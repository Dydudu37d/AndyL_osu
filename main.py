import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import transforms
import cv2
import numpy as np
from PIL import Image
import os
import random
from torch.utils.data import Dataset, DataLoader
import keyboard
import pyautogui
import time
from mss import mss  # ç”¨äºå±å¹•æ•è·
import math

pyautogui.FAILSAFE = False

# ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
ROOT = os.path.dirname(os.path.abspath(__file__))
MODELS_PATH = os.path.join(ROOT, 'models')
TRAIN_IMG_PATH = os.path.join(ROOT, 'train_img')
TEST_IMG_PATH = os.path.join(ROOT, 'test_img')
RESULTS_PATH = os.path.join(ROOT, 'results')

for path in [MODELS_PATH, RESULTS_PATH]:
    os.makedirs(path, exist_ok=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å…¨å±€é…ç½®å‚æ•°
CONFIG = {
    'image_size': (160, 90),  # 16:9å®½å±å°ºå¯¸
    'num_classes': 4,
    'num_anchors': 4,
    'confidence_threshold': 0.5,
    'nms_threshold': 0.45,
    'batch_size': 1024,
    'num_epochs': 500,
    'learning_rate': 0.001,
    # ç±»åˆ«åç§°æ˜ å°„
    'class_names': ['circle', 'slider', 'spinner', 'back']
}

os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å¼µGPU
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,garbage_collection_threshold:0.8'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # èª¿è©¦ç”¨

print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

class OsuNet(nn.Module):
    """Osuæ¸¸æˆå¯¹è±¡åˆ†ç±»ç½‘ç»œ - ä¼˜åŒ–ï¼šç®€åŒ–ç½‘ç»œç»“æ„ï¼Œå‡å°‘è®¡ç®—é‡"""
    def __init__(self, num_classes=CONFIG['num_classes']):
        super().__init__()
        self.num_classes = num_classes
        
        # ç®€åŒ–çš„å·ç§¯éª¨å¹²ç½‘ç»œï¼Œå‡å°‘é€šé“æ•°å’Œå±‚æ•°
        self.backbone = nn.Sequential(
            # è¾“å…¥ 3x160x90 -> è¾“å‡º 16x80x45
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # è¾“å…¥ 16x80x45 -> è¾“å‡º 32x40x22
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # è¾“å…¥ 32x40x22 -> è¾“å‡º 64x20x11
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # è¾“å…¥ 64x20x11 -> è¾“å‡º 128x10x5
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        
        # ç®€åŒ–çš„åˆ†ç±»å¤´ï¼Œå‡å°‘ç¥ç»å…ƒæ•°é‡å’Œdropoutå±‚
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),  # å…¨å±€å¹³å‡æ± åŒ–ï¼Œè¾“å‡º [batch_size, 128, 1, 1]
            nn.Flatten(),  # å±•å¹³ä¸º [batch_size, 128]
            nn.Linear(128, 64),  # å‡å°‘ç¥ç»å…ƒæ•°é‡
            nn.ReLU(inplace=True),
            nn.Linear(64, num_classes)  # ç›´æ¥è¾“å‡ºåˆ†ç±»ç»“æœ
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """ä¼˜åŒ–çš„æƒé‡åˆå§‹åŒ–æ–¹æ³•"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # ä½¿ç”¨Kaimingåˆå§‹åŒ–ä»£æ›¿é»˜è®¤çš„æ­£æ€åˆ†å¸ƒï¼Œæ›´é€‚åˆReLUæ¿€æ´»å‡½æ•°
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        features = self.backbone(x)
        predictions = self.classifier(features)
        return predictions


def create_anchor_boxes(feature_size=None, image_size=CONFIG['image_size']):
    """åˆ›å»ºé”šç‚¹æ¡†ï¼ŒåŸºäºç‰¹å¾å›¾å°ºå¯¸"""
    anchors = []
    # ä¸åŒå°ºå¯¸å’Œå®½é«˜æ¯”çš„é”šç‚¹æ¡†ï¼Œè€ƒè™‘16:9æ¯”ä¾‹
    base_sizes = [(0.1, 0.1), (0.2, 0.2), (0.3, 0.3), (0.2, 0.1), (0.1, 0.2)]  # æ·»åŠ ä¸åŒå®½é«˜æ¯”çš„é”šæ¡†
    
    for size in base_sizes:
        w, h = size[0] * image_size[0], size[1] * image_size[1]
        anchors.append([w, h])
    
    return torch.tensor(anchors).float()  # [5, 2]


def decode_predictions(predictions, anchor_boxes, image_size=CONFIG['image_size'], num_classes=CONFIG['num_classes']):
    """
    å°†ç½‘ç»œè¾“å‡ºè§£ç ä¸ºå®é™…çš„è¾¹ç•Œæ¡†
    predictions: [batch, A*(4+1+C), H, W]
    è¿”å›: è¾¹ç•Œæ¡†, ç½®ä¿¡åº¦, ç±»åˆ«æ¦‚ç‡
    """
    batch_size, _, grid_h, grid_w = predictions.shape
    num_anchors = anchor_boxes.shape[0]
    
    # é‡å¡‘é¢„æµ‹å€¼ [batch, A, 5+C, H, W]
    pred = predictions.view(batch_size, num_anchors, -1, grid_h, grid_w)
    
    # æå–å„ä¸ªåˆ†é‡
    bbox_deltas = pred[:, :, :4, :, :]  # [batch, A, 4, H, W]
    objectness = torch.sigmoid(pred[:, :, 4:5, :, :])  # ç‰©ä½“ç½®ä¿¡åº¦
    class_probs = torch.softmax(pred[:, :, 5:5+num_classes, :, :], dim=2)  # ç±»åˆ«æ¦‚ç‡ - ç¡®ä¿æ­£ç¡®çš„åˆ‡ç‰‡
    
    # è®¡ç®—ç½‘æ ¼ä¸­å¿ƒåæ ‡
    grid_y, grid_x = torch.meshgrid(torch.arange(grid_h), torch.arange(grid_w))
    grid_x = grid_x.to(predictions.device).float()
    grid_y = grid_y.to(predictions.device).float()
    
    # è®¡ç®—çœŸå®è¾¹ç•Œæ¡†åæ ‡ - åˆ†åˆ«è€ƒè™‘å®½åº¦å’Œé«˜åº¦
    stride_h = image_size[1] / grid_h
    stride_w = image_size[0] / grid_w
    
    # è§£ç è¾¹ç•Œæ¡†åæ ‡
    box_centers_x = (torch.sigmoid(bbox_deltas[:, :, 0, :, :]) + grid_x) * stride_w
    box_centers_y = (torch.sigmoid(bbox_deltas[:, :, 1, :, :]) + grid_y) * stride_h
    box_widths = torch.exp(bbox_deltas[:, :, 2, :, :]) * anchor_boxes[:, 0].view(1, -1, 1, 1)
    box_heights = torch.exp(bbox_deltas[:, :, 3, :, :]) * anchor_boxes[:, 1].view(1, -1, 1, 1)
    
    # è½¬æ¢ä¸ºå·¦ä¸Šè§’å’Œå³ä¸‹è§’åæ ‡
    x1 = box_centers_x - box_widths / 2
    y1 = box_centers_y - box_heights / 2
    x2 = box_centers_x + box_widths / 2
    y2 = box_centers_y + box_heights / 2
    
    # ç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒèŒƒå›´å†…
    x1 = torch.clamp(x1, 0, image_size[0] - 1)
    y1 = torch.clamp(y1, 0, image_size[1] - 1)
    x2 = torch.clamp(x2, 0, image_size[0] - 1)
    y2 = torch.clamp(y2, 0, image_size[1] - 1)
    
    boxes = torch.stack([x1, y1, x2, y2], dim=2)  # [batch, A, 4, H, W]
    
    return boxes, objectness, class_probs


def non_max_suppression(boxes, scores, threshold=CONFIG['nms_threshold']):
    """éæå¤§å€¼æŠ‘åˆ¶ï¼Œè¿‡æ»¤é‡å çš„è¾¹ç•Œæ¡†"""
    if len(boxes) == 0:
        return []
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå¤„ç†
    boxes_np = boxes.cpu().numpy()
    scores_np = scores.cpu().numpy()
    
    # è®¡ç®—è¾¹ç•Œæ¡†é¢ç§¯
    x1 = boxes_np[:, 0]
    y1 = boxes_np[:, 1]
    x2 = boxes_np[:, 2]
    y2 = boxes_np[:, 3]
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    
    # æŒ‰åˆ†æ•°æ’åº
    order = scores_np.argsort()[::-1]
    
    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        
        # è®¡ç®—ä¸å…¶ä»–è¾¹ç•Œæ¡†çš„é‡å åŒºåŸŸ
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        
        # è®¡ç®—IoU
        ovr = inter / (areas[i] + areas[order[1:]] - inter)
        
        # ä¿ç•™IoUå°äºé˜ˆå€¼çš„è¾¹ç•Œæ¡†
        inds = np.where(ovr <= threshold)[0]
        order = order[inds + 1]
    
    return keep


def detect_objects(model, image_path, confidence_threshold=CONFIG['confidence_threshold'], 
                  device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):
    """åœ¨å•å¼ å›¾åƒä¸Šè¿›è¡Œå¯¹è±¡æ£€æµ‹"""
    model.eval()
    model.to(device)
    
    # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    original_size = image.size
    resized_image = transforms.Resize(CONFIG['image_size'])(image)
    image_tensor = transforms.ToTensor()(resized_image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        predictions = model(image_tensor)
    
    # è§£ç é¢„æµ‹ - ç¡®ä¿ä¼ å…¥æ­£ç¡®çš„ç±»åˆ«æ•°
    anchor_boxes = create_anchor_boxes().to(device)
    boxes, objectness, class_probs = decode_predictions(predictions, anchor_boxes, CONFIG['image_size'][0])
    
    # è½¬æ¢ä¸ºå¹³é¢æ ¼å¼
    batch_size, num_anchors, _, grid_h, grid_w = boxes.shape
    boxes = boxes.view(batch_size, -1, 4)  # [batch, A*H*W, 4]
    objectness = objectness.view(batch_size, -1)  # [batch, A*H*W]
    class_probs = class_probs.view(batch_size, -1, CONFIG['num_classes'])  # [batch, A*H*W, C]
    
    detected_boxes = []
    detected_scores = []
    detected_classes = []
    
    # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡ï¼ˆè¿™é‡Œåªæœ‰ä¸€ä¸ªå›¾åƒï¼‰
    for b in range(batch_size):
        # è·å–é«˜ç½®ä¿¡åº¦çš„æ£€æµ‹
        high_conf_indices = (objectness[b] > confidence_threshold).nonzero().squeeze(1)
        
        if len(high_conf_indices) == 0:
            continue
        
        # æå–é«˜ç½®ä¿¡åº¦çš„è¾¹ç•Œæ¡†å’Œåˆ†æ•°
        high_conf_boxes = boxes[b][high_conf_indices]
        high_conf_scores = objectness[b][high_conf_indices]
        high_conf_classes = class_probs[b][high_conf_indices].argmax(dim=1)
        
        # åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶
        keep_indices = non_max_suppression(high_conf_boxes, high_conf_scores)
        
        # ä¿å­˜ç»“æœ
        for idx in keep_indices:
            box = high_conf_boxes[idx]
            # è½¬æ¢å›åŸå§‹å›¾åƒå°ºå¯¸
            scale_x = original_size[0] / CONFIG['image_size'][0]
            scale_y = original_size[1] / CONFIG['image_size'][1]
            box_scaled = [
                box[0].item() * scale_x,
                box[1].item() * scale_y,
                box[2].item() * scale_x,
                box[3].item() * scale_y
            ]
            detected_boxes.append(box_scaled)
            detected_scores.append(high_conf_scores[idx].item())
            detected_classes.append(high_conf_classes[idx].item())
    
    return detected_boxes, detected_scores, detected_classes


class OsuDataset(Dataset):
    """Osuæ¸¸æˆå¯¹è±¡æ£€æµ‹æ•°æ®é›†"""
    def __init__(self, img_paths=None, labels=None, img_dir=None, transform=None, image_size=CONFIG['image_size'], augment=False):
        self.image_size = image_size
        self.num_classes = CONFIG['num_classes']
        self.class_names = CONFIG['class_names']
        self.img_paths = []
        self.labels = []
        
        # å®šä¹‰å¢å¼ºçš„æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†è½¬æ¢
        if augment:
            self.transform = transforms.Compose([
                transforms.RandomRotation(15),  # å¢åŠ æ—‹è½¬è§’åº¦
                transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),  # éšæœºè£å‰ªå’Œç¼©æ”¾
                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # å¢å¼ºé¢œè‰²æŠ–åŠ¨
                transforms.RandomHorizontalFlip(p=0.5),  # å¢åŠ æ°´å¹³ç¿»è½¬æ¦‚ç‡
                transforms.RandomVerticalFlip(p=0.2),  # æ·»åŠ å‚ç›´ç¿»è½¬
                transforms.RandomGrayscale(p=0.1),  # æ·»åŠ ç°åº¦å˜æ¢
                transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),  # æ·»åŠ é«˜æ–¯æ¨¡ç³Š
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transform
            if self.transform is None:
                self.transform = transforms.Compose([
                    transforms.Resize(image_size),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                ])
        
        # æ”¯æŒç›´æ¥ä¼ å…¥å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        if img_paths is not None:
            self.img_paths = img_paths
            self.labels = labels if labels is not None else [0] * len(self.img_paths)
        # æ”¯æŒåˆ†ç±»æ–‡ä»¶å¤¹ç»“æ„
        elif img_dir and os.path.exists(img_dir):
            self._load_images(img_dir)
        # å…¼å®¹æ—§ç‰ˆæœ¬çš„å‚æ•°æ ¼å¼
        elif img_dir and isinstance(img_dir, list):
            self.img_paths = img_dir
            self.labels = labels if labels is not None else [0] * len(self.img_paths)
    
    def _load_images(self, img_dir):
        """åŠ è½½å›¾åƒè·¯å¾„å’Œæ ‡ç­¾"""
        for root, dirs, files in os.walk(img_dir):
            for file in files:
                if file.endswith(('.jpg', '.jpeg', '.png')):
                    img_path = os.path.join(root, file)
                    self.img_paths.append(img_path)
                    
                    # ä»æ–‡ä»¶å¤¹åç§°è·å–æ ‡ç­¾
                    label = os.path.basename(root)
                    # å°†æ ‡ç­¾æ˜ å°„ä¸ºç±»åˆ«ç´¢å¼•
                    try:
                        # å°è¯•å°†æ–‡ä»¶å¤¹åä½œä¸ºç±»åˆ«ç´¢å¼•
                        class_idx = int(label) - 1  # å‡è®¾æ–‡ä»¶å¤¹æ˜¯1,2,3...
                        # ç¡®ä¿ç±»åˆ«ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                        class_idx = max(0, min(class_idx, self.num_classes - 1))
                        self.labels.append(class_idx)
                    except ValueError:
                        # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•´æ•°ï¼Œå°è¯•åŒ¹é…ç±»åˆ«åç§°
                        if label in self.class_names:
                            self.labels.append(self.class_names.index(label))
                        else:
                            # é»˜è®¤ä½¿ç”¨0ç±»åˆ«
                            self.labels.append(0)
    
    def __len__(self):
        return len(self.img_paths)
    
    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        try:
            image = Image.open(img_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            # è¿”å›å›¾åƒå’Œå¯¹åº”çš„æ ‡ç­¾ç´¢å¼•
            return image, self.labels[idx]
        except Exception as e:
            print(f"åŠ è½½å›¾åƒ {img_path} å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å›¾åƒå’Œæ ‡ç­¾
            default_image = torch.zeros(3, self.image_size[1], self.image_size[0])
            return default_image, 0


def load_train_imgs(img_path):
    """åŠ è½½è®­ç»ƒå›¾åƒå’Œæ ‡ç­¾"""
    if not os.path.exists(img_path):
        print(f"è­¦å‘Š: è®­ç»ƒå›¾åƒè·¯å¾„ä¸å­˜åœ¨: {img_path}")
        return [], []
    
    # å®šä¹‰å›¾åƒè½¬æ¢ - å¢å¼ºç‰ˆæœ¬ï¼Œå¢åŠ å¤šç§æ•°æ®å¢å¼º
    transform = transforms.Compose([
        transforms.Resize(CONFIG['image_size']),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.RandomAffine(degrees=0, scale=(0.8, 1.2)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.RandomGrayscale(p=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    dataset = OsuDataset(img_dir=img_path, transform=transform)
    imgs = [dataset[i][0] for i in range(len(dataset))]
    tags = dataset.labels
    
    return imgs, tags


def train_model(model, train_loader, num_epochs=CONFIG['num_epochs'], 
                             learning_rate=CONFIG['learning_rate'], device=None):
    """GPUä¼˜åŒ–çš„è®­ç»ƒå‡½æ•° - ä¼˜åŒ–spinnerè¯†åˆ«"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model.to(device)
    model.train()
    
    print(f"ğŸš€ å¼€å§‹GPUè®­ç»ƒ on {device}")
    print(f"ğŸ“Š æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
    
    # ä½¿ç”¨ä¼˜åŒ–çš„ä¼˜åŒ–å™¨é…ç½®ï¼Œæ·»åŠ weight_decayæ­£åˆ™åŒ–
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    # ä½¿ç”¨æ›´å¹³æ»‘çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)
    
    # ç”¨äºç›‘æ§GPUä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
    
    # è°ƒæ•´ç±»åˆ«æƒé‡ï¼Œé‡ç‚¹å…³æ³¨è¡¨ç°æœ€å·®çš„ç±»åˆ«
    # åŸºäºè¯„ä¼°ç»“æœï¼Œsliderå’Œspinnerè¡¨ç°ä¸º0ï¼Œéœ€è¦å¤§å¹…å¢åŠ æƒé‡
# è®¾ç½®ç±»åˆ«æƒé‡ï¼Œè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    class_weights = torch.tensor([1.0, 3.0, 3.0, 3.0], device=device)  # å¤§å¹…å¢åŠ sliderã€spinnerå’Œbackç±»åˆ«çš„æƒé‡
    
    # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œè®©æ¨¡å‹æœ‰æ›´å¤šæ—¶é—´å­¦ä¹ å¤æ‚ç‰¹å¾
    num_epochs = max(num_epochs, 50)  # ç¡®ä¿è‡³å°‘è®­ç»ƒ50è½®
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        batch_count = 0
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            # ç¡®ä¿æ•°æ®åœ¨GPUä¸Š
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            
            # è®¡ç®—åˆ†ç±»æŸå¤±ï¼Œä½¿ç”¨ç±»åˆ«æƒé‡
            loss = torch.nn.functional.cross_entropy(
                outputs,  # æ–°æ¨¡å‹ç›´æ¥è¾“å‡ºåˆ†ç±»ç»“æœ [batch_size, num_classes]
                labels,
                weight=class_weights  # æ·»åŠ ç±»åˆ«æƒé‡
            )
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad(set_to_none=True)  # æ›´å¿«çš„æ¢¯åº¦æ¸…é›¶
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            batch_count += 1
            
            # æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¯é€‰ï¼‰
            if batch_idx % 50 == 0 and torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1024**2
                print(f"  æ‰¹æ¬¡ {batch_idx}, GPUå†…å­˜: {memory_allocated:.1f}MB")
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        avg_loss = epoch_loss / batch_count
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 20 == 0:  # æ›´é¢‘ç¹åœ°ä¿å­˜æ£€æŸ¥ç‚¹
            model_path = os.path.join(MODELS_PATH, f'osu_model_temp.pth')
            torch.save(model.state_dict(), model_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    
    model_path = os.path.join(MODELS_PATH, f'osu_model.pth')
    torch.save(model.state_dict(), model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    
    return model

def evaluate_model(model, test_dir, class_names=None):
    """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
    model.eval()
    
    if not os.path.exists(test_dir):
        print(f"è­¦å‘Š: æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_dir}")
        return
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    os.makedirs(os.path.join(RESULTS_PATH, 'detections'), exist_ok=True)
    
    # è·å–æµ‹è¯•å›¾åƒ
    test_images = []
    for file in os.listdir(test_dir):
        if file.endswith(('.jpg', '.jpeg', '.png')):
            test_images.append(os.path.join(test_dir, file))
    
    # å¯¹æ¯å¼ å›¾åƒè¿›è¡Œæ£€æµ‹
    for img_path in test_images:
        boxes, scores, classes = detect_objects(model, img_path)
        
        # å¯è§†åŒ–å¹¶ä¿å­˜ç»“æœ
        img_name = os.path.basename(img_path)
        save_path = os.path.join(RESULTS_PATH, 'detections', f'detected_{img_name}')
        visualize_detection(img_path, boxes, scores, classes, class_names, save_path)
    
    print(f"è¯„ä¼°å®Œæˆï¼æ£€æµ‹ç»“æœå·²ä¿å­˜è‡³: {os.path.join(RESULTS_PATH, 'detections')}")


def generate_dataset(output_dir, num_samples_per_class=300, image_size=CONFIG['image_size']):
    """ç”ŸæˆOsuæ¸¸æˆå¯¹è±¡æ£€æµ‹æ•°æ®é›†"""
    os.makedirs(output_dir, exist_ok=True)
    
    # å®šä¹‰ä¸åŒç±»åˆ«çš„è¾“å‡ºç›®å½•
    classes_config = {
        '1': {'name': 'circle', 'generate_func': generate_circle},
        '2': {'name': 'slider', 'generate_func': generate_slider},
        '3': {'name': 'spinner', 'generate_func': generate_spinner}
    }
    
    total_samples = 0
    
    # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆæ ·æœ¬
    for class_id, class_config in classes_config.items():
        class_dir = os.path.join(output_dir, class_id)
        os.makedirs(class_dir, exist_ok=True)
        
        print(f"ç”Ÿæˆç±»åˆ« {class_config['name']} çš„æ ·æœ¬...")
        for i in range(num_samples_per_class):
            # åˆ›å»ºç©ºç™½å›¾åƒ
            image = np.ones((image_size[1], image_size[0], 3), dtype=np.uint8) * 255
            
            # è°ƒç”¨å¯¹åº”çš„ç”Ÿæˆå‡½æ•°
            image = class_config['generate_func'](image, image_size)
            
            # æ·»åŠ ä¸€äº›å™ªå£°
            if random.random() > 0.5:
                noise = np.random.randint(0, 50, (image_size[1], image_size[0], 3), dtype=np.uint8)
                image = cv2.add(image, noise)
            
            # ä¿å­˜å›¾åƒ
            img_path = os.path.join(class_dir, f"{class_config['name']}_{i:05d}.png")
            cv2.imwrite(img_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
            
            if (i + 1) % 100 == 0:
                print(f"  å·²ç”Ÿæˆ {i + 1}/{num_samples_per_class} ä¸ª{class_config['name']}æ ·æœ¬")
        
        total_samples += num_samples_per_class
    
    print(f"æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {total_samples} ä¸ªæ ·æœ¬ï¼Œä¿å­˜åœ¨: {output_dir}")

def generate_circle(image, image_size):
    """ç”Ÿæˆåœ†å½¢ç›®æ ‡"""
    # éšæœºç”Ÿæˆåœ†å½¢å‚æ•°
    radius = random.randint(max(8, image_size[1]//10), min(30, image_size[0]//3))
    center_x = random.randint(radius, image_size[0] - radius)
    center_y = random.randint(radius, image_size[1] - radius)
    
    # éšæœºé¢œè‰²ï¼ˆçº¢è‰²ç³»åˆ—ï¼‰
    color = (random.randint(200, 255), random.randint(0, 100), random.randint(0, 100))
    
    # ç»˜åˆ¶åœ†å½¢
    cv2.circle(image, (center_x, center_y), radius, color, -1)
    
    # æ·»åŠ å†…éƒ¨å°åœ†ä½œä¸ºç»†èŠ‚
    if random.random() > 0.3:
        inner_radius = radius // 3
        inner_color = (255, 255, 255)
        cv2.circle(image, (center_x, center_y), inner_radius, inner_color, -1)
    
    return image

def generate_slider(image, image_size):
    """ç”Ÿæˆæ»‘å—ç›®æ ‡"""
    # éšæœºç”Ÿæˆæ»‘å—å‚æ•°
    width = random.randint(10, 20)
    length = random.randint(30, 60)
    angle = random.randint(0, 180) * np.pi / 180  # è½¬æ¢ä¸ºå¼§åº¦
    
    # è®¡ç®—èµ·ç‚¹å’Œç»ˆç‚¹
    center_x = random.randint(length//2 + 10, image_size[0] - length//2 - 10)
    center_y = random.randint(width//2 + 10, image_size[1] - width//2 - 10)
    
    # æ»‘å—é¢œè‰²ï¼ˆè“è‰²ç³»åˆ—ï¼‰
    color = (random.randint(0, 100), random.randint(0, 100), random.randint(200, 255))
    
    # åˆ›å»ºæ—‹è½¬çŸ©é˜µ
    rotation_mat = cv2.getRotationMatrix2D((center_x, center_y), angle * 180 / np.pi, 1)
    
    # åˆ›å»ºçŸ©å½¢
    x1 = center_x - length//2
    y1 = center_y - width//2
    x2 = x1 + length
    y2 = y1 + width
    pts = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]], dtype=np.float32)
    
    # åº”ç”¨æ—‹è½¬
    rotated_pts = cv2.transform(pts.reshape(-1, 1, 2), rotation_mat).reshape(-1, 2)
    
    # ç»˜åˆ¶æ—‹è½¬åçš„çŸ©å½¢
    cv2.fillPoly(image, [rotated_pts.astype(np.int32)], color)
    
    # æ·»åŠ æ»‘å—è½¨é“æŒ‡ç¤º
    if random.random() > 0.5:
        # è®¡ç®—è½¨é“æ–¹å‘
        dx = np.cos(angle) * length * 0.8
        dy = np.sin(angle) * length * 0.8
        
        # ç»˜åˆ¶è½¨é“çº¿
        cv2.line(image, (center_x, center_y), 
                (int(center_x + dx), int(center_y + dy)), 
                (255, 255, 255), 2)
    
    return image

def generate_spinner(image, image_size):
    """ç”Ÿæˆæ—‹è½¬å™¨ç›®æ ‡"""
    # éšæœºç”Ÿæˆæ—‹è½¬å™¨å‚æ•°
    radius = random.randint(max(15, image_size[1]//8), min(40, image_size[0]//4))
    center_x = random.randint(radius + 10, image_size[0] - radius - 10)
    center_y = random.randint(radius + 10, image_size[1] - radius - 10)
    
    # æ—‹è½¬å™¨é¢œè‰²ï¼ˆç´«è‰²ç³»åˆ—ï¼‰
    color = (random.randint(200, 255), random.randint(0, 100), random.randint(200, 255))
    
    # ç»˜åˆ¶å¤–åœ†
    cv2.circle(image, (center_x, center_y), radius, color, 3)
    
    # ç»˜åˆ¶å†…åœ†
    inner_radius = radius // 2
    cv2.circle(image, (center_x, center_y), inner_radius, color, 2)
    
    # ç»˜åˆ¶æ—‹è½¬æŒ‡ç¤ºçº¿
    for i in range(4):
        angle = i * np.pi / 2 + random.uniform(0, np.pi/4)  # éšæœºæ—‹è½¬ä¸€ç‚¹
        x1 = center_x + int(np.cos(angle) * inner_radius)
        y1 = center_y + int(np.sin(angle) * inner_radius)
        x2 = center_x + int(np.cos(angle) * radius)
        y2 = center_y + int(np.sin(angle) * radius)
        cv2.line(image, (x1, y1), (x2, y2), color, 2)
    
    # æ·»åŠ ä¸­å¿ƒç‚¹
    cv2.circle(image, (center_x, center_y), 3, (255, 255, 255), -1)
    
    return image


def main():
    """ä¸»å‡½æ•° - è¿è¡Œå®Œæ•´çš„å¤šç±»åˆ«å¯¹è±¡æ£€æµ‹æµç¨‹"""
    # 1. åˆ›å»ºæ¨¡å‹ - é…ç½®ä¸º3ä¸ªç±»åˆ«
    model = OsuNet(num_classes=CONFIG['num_classes'])
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 2. å‡†å¤‡æ•°æ®é›†
    TRAIN_IMG_PATH = 'train_img'
    TEST_IMG_PATH = 'test_img'
    
    # æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯
    print("===== é…ç½®ä¿¡æ¯ =====")
    print(f"å›¾åƒå¤§å°: {CONFIG['image_size']}")
    print(f"ç±»åˆ«æ•°é‡: {CONFIG['num_classes']}")
    print(f"ç±»åˆ«åç§°: {CONFIG['class_names']}")
    print(f"æ‰¹é‡å¤§å°: {CONFIG['batch_size']}")
    print(f"è®­ç»ƒè½®æ•°: {CONFIG['num_epochs']}")
    print("==================\n")
    
    # æ£€æŸ¥è®­ç»ƒç›®å½•æ˜¯å¦å­˜åœ¨å¹¶åŒ…å«ç±»åˆ«æ–‡ä»¶å¤¹
    need_generate = False
    if not os.path.exists(TRAIN_IMG_PATH):
        need_generate = True
    else:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«1ã€2ã€3ä¸‰ä¸ªç±»åˆ«æ–‡ä»¶å¤¹
        has_all_classes = all(os.path.exists(os.path.join(TRAIN_IMG_PATH, str(i))) for i in [1, 2, 3])
        if not has_all_classes:
            print("è®­ç»ƒç›®å½•ç¼ºå°‘éƒ¨åˆ†ç±»åˆ«æ–‡ä»¶å¤¹")
            need_generate = True
    
    # å¦‚æœéœ€è¦ï¼Œç”Ÿæˆæ•°æ®é›†
    if need_generate:
        print("ç”Ÿæˆè®­ç»ƒæ•°æ®é›†...")
        generate_dataset(TRAIN_IMG_PATH, num_samples_per_class=300)
    else:
        print(f"ä½¿ç”¨ç°æœ‰è®­ç»ƒæ•°æ®: {TRAIN_IMG_PATH}")
    
    # 3. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨å¢å¼ºç‰ˆæœ¬çš„OsuDataset
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    
    # ä½¿ç”¨å¢å¼ºçš„æ•°æ®å¢å¼º
    train_dataset = OsuDataset(img_dir=TRAIN_IMG_PATH, augment=True)
    
    if len(train_dataset) == 0:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå›¾åƒï¼")
        return
    
    # æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒç»Ÿè®¡
    from collections import Counter
    tag_counts = Counter(train_dataset.labels)
    print("æ•°æ®åˆ†å¸ƒ:")
    for class_idx, count in tag_counts.items():
        class_name = CONFIG['class_names'][min(class_idx, len(CONFIG['class_names']) - 1)]
        print(f"  {class_name}: {count} å¼ å›¾åƒ")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œæ·»åŠ shuffleå’Œnum_workers
    train_loader = DataLoader(
        train_dataset, 
        batch_size=CONFIG['batch_size'], 
        shuffle=True,  # ç¡®ä¿æ•°æ®éšæœºæ‰“ä¹±
        num_workers=0,  # å¤šçº¿ç¨‹åŠ è½½æ•°æ®
        pin_memory=True  # åŠ é€ŸGPUæ•°æ®ä¼ è¾“
    )
    
    # 5. å°è¯•åŠ è½½ç°æœ‰æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    model_path = os.path.join(MODELS_PATH, f'osu_model.pth')
    if os.path.exists(model_path):
        try:
            model.load_state_dict(torch.load(model_path))
            print(f"âœ… å·²åŠ è½½ç°æœ‰æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹å¤±è´¥: {e}ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # 6. è®­ç»ƒæ¨¡å‹ - å–æ¶ˆæ³¨é‡Šï¼Œç¡®ä¿æ¨¡å‹è¢«è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    model = train_model(model, train_loader, device=device)
    
    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(MODELS_PATH, f'osu_model.pth')
    torch.save(model.state_dict(), final_model_path)
    print(f"ğŸ‰ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")
    
    # 8. è¯„ä¼°æ¨¡å‹æ€§èƒ½
    print("\n===== å¼€å§‹æ¨¡å‹è¯„ä¼° =====")
    # ç›´æ¥åœ¨main.pyä¸­å®ç°è¯„ä¼°é€»è¾‘ï¼Œé¿å…subprocessç¯å¢ƒé—®é¢˜
    from PIL import Image
    import torchvision.transforms as transforms
    from collections import Counter, defaultdict
    
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # å®šä¹‰è¯„ä¼°æ•°æ®ç›®å½•
    eval_dir = TRAIN_IMG_PATH
    
    # å®šä¹‰å›¾åƒè½¬æ¢
    transform = transforms.Compose([
        transforms.Resize(CONFIG['image_size']),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½è¯„ä¼°å›¾åƒ
    eval_dataset = OsuDataset(img_dir=eval_dir, transform=transform)
    eval_loader = DataLoader(eval_dataset, batch_size=CONFIG['batch_size'], shuffle=False)
    
    # è¯„ä¼°å˜é‡
    correct = 0
    total = 0
    class_correct = [0] * CONFIG['num_classes']
    class_total = [0] * CONFIG['num_classes']
    confusion_matrix = [[0 for _ in range(CONFIG['num_classes'])] for _ in range(CONFIG['num_classes'])]
    
    # æ— æ¢¯åº¦è¯„ä¼°
    with torch.no_grad():
        for images, labels in eval_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # ç±»åˆ«çº§ç»Ÿè®¡
            for i in range(len(labels)):
                label = labels[i]
                pred = predicted[i]
                confusion_matrix[label.item()][pred.item()] += 1
                if label == pred:
                    class_correct[label.item()] += 1
                class_total[label.item()] += 1
    
    # è®¡ç®—æŒ‡æ ‡
    overall_accuracy = 100 * correct / total
    print(f"æ€»ä½“å‡†ç¡®ç‡: {overall_accuracy:.2f}%")
    
    print("\nç±»åˆ«çº§å‡†ç¡®ç‡:")
    for i in range(CONFIG['num_classes']):
        if class_total[i] > 0:
            accuracy = 100 * class_correct[i] / class_total[i]
            print(f"  {CONFIG['class_names'][i]}: {accuracy:.2f}% ({class_correct[i]}/{class_total[i]})")
    
    print("\næ··æ·†çŸ©é˜µ:")
    print("          " + "  ".join([f"{name:>8}" for name in CONFIG['class_names']]))
    for i in range(CONFIG['num_classes']):
        row = [f"{confusion_matrix[i][j]:8d}" for j in range(CONFIG['num_classes'])]
        print(f"{CONFIG['class_names'][i]:>8}: {''.join(row)}")
    
    print("\n===== è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ =====")
    
    run(model)

def capture_screen():
    """æ•è·å½“å‰å±å¹•å›¾åƒ - ä¼˜åŒ–ï¼šåªæ•è·ä¸­å¿ƒåŒºåŸŸ"""
    try:
        with mss() as sct:
            # è·å–ä¸»æ˜¾ç¤ºå™¨
            monitor = sct.monitors[1]  # ä¸»æ˜¾ç¤ºå™¨
            
            # åªæ•è·å±å¹•ä¸­å¿ƒåŒºåŸŸï¼Œå‡å°‘å¤„ç†æ•°æ®é‡
            width = monitor['width']
            height = monitor['height']
            crop_width = int(width)
            crop_height = int(height)
            
            # è®¡ç®—ä¸­å¿ƒåŒºåŸŸåæ ‡
            left = int((width - crop_width) / 2)
            top = int((height - crop_height) / 2)
            
            # åˆ›å»ºè‡ªå®šä¹‰ç›‘æ§åŒºåŸŸ
            crop_monitor = {
                "left": left,
                "top": top,
                "width": crop_width,
                "height": crop_height
            }
            
            screenshot = sct.grab(crop_monitor)
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            img = np.array(screenshot)
            # è½¬æ¢ä¸ºRGBæ ¼å¼ï¼ˆmssé»˜è®¤è¿”å›BGRAï¼‰
            img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)
            return img
    except Exception as e:
        print(f"å±å¹•æ•è·å¤±è´¥: {e}")
        # è¿”å›ä¸€ä¸ªç©ºç™½å›¾åƒä½œä¸ºå¤‡ç”¨
        return np.ones((720, 1280, 3), dtype=np.uint8) * 255

def preprocess_image(image):
    """é¢„å¤„ç†å›¾åƒä»¥ä¾›æ¨¡å‹è¾“å…¥ - ä¼˜åŒ–ï¼šç®€åŒ–æµç¨‹ï¼Œç›´æ¥ä½¿ç”¨OpenCVå¤„ç†"""
    # è°ƒæ•´å›¾åƒå¤§å°
    resized = cv2.resize(image, CONFIG['image_size'])
    # ç›´æ¥è½¬æ¢ä¸ºå¼ é‡ï¼Œé¿å…PILè½¬æ¢
    img_tensor = torch.from_numpy(resized).permute(2, 0, 1).float() / 255.0
    # æ ‡å‡†åŒ–
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    img_tensor = (img_tensor - mean) / std
    return img_tensor

def parse_predictions(predictions, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):
    """è§£æåˆ†ç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœ - ä¼˜åŒ–ï¼šç®€åŒ–è®¡ç®—ï¼Œå‡å°‘ä¸å¿…è¦çš„æ“ä½œ"""
    results = []
    
    # åˆ†ç±»æ¨¡å‹è¾“å‡º [batch_size, num_classes]
    # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ
    class_scores = predictions[0]  # [num_classes]
    
    # è®¡ç®—ç±»åˆ«æ¦‚ç‡ï¼ˆä½¿ç”¨softmaxï¼‰
    class_probs = torch.softmax(class_scores, dim=0)  # [num_classes]
    
    # æ‰¾åˆ°æœ€å¤§æ¦‚ç‡çš„ç±»åˆ«
    max_prob, predicted_class = torch.max(class_probs, dim=0)
    
    # ä¸ºä¸åŒç±»åˆ«è®¾ç½®ä¸åŒçš„æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
    if predicted_class.item() == 0:  # circle
        min_confidence = 0.6  # circleéœ€è¦è¾ƒé«˜ç½®ä¿¡åº¦
    elif predicted_class.item() == 1:  # slider
        min_confidence = 0.7  # slideréœ€è¦æ›´é«˜ç½®ä¿¡åº¦ï¼Œå‡å°‘è¯¯åˆ¤
    elif predicted_class.item() == 2:  # spinner
        min_confidence = 0.8  # spinneréœ€è¦æœ€é«˜ç½®ä¿¡åº¦
    elif predicted_class.item() == 3:
        min_confidence = 0.7
    else:
        min_confidence = CONFIG['confidence_threshold']
    
    if max_prob.item() < min_confidence:
        return results  # ç½®ä¿¡åº¦ä¸è¶³ï¼Œè¿”å›ç©ºåˆ—è¡¨
    
    # ç®€åŒ–è¾¹ç•Œæ¡†è®¡ç®—ï¼Œç›´æ¥è¿”å›ä¸­å¿ƒåŒºåŸŸ
    image_w, image_h = CONFIG['image_size']
    
    # ä½¿ç”¨å›ºå®šçš„è¾¹ç•Œæ¡†å¤§å°ï¼Œå‡å°‘è®¡ç®—
    center_w = image_w * 0.5
    center_h = image_h * 0.5
    x1 = int((image_w - center_w) / 2)
    y1 = int((image_h - center_h) / 2)
    x2 = int(x1 + center_w)
    y2 = int(y1 + center_h)
    
    # åˆ›å»ºè¾¹ç•Œæ¡†
    box = [x1, y1, x2, y2]
    
    # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
    results.append({
        'box': box,
        'score': max_prob.item(),
        'class': predicted_class.item(),
        'class_name': CONFIG['class_names'][predicted_class.item()]
    })
    
    return results

def visualize_results(screen, results):
    """åœ¨å±å¹•å›¾åƒä¸Šå¯è§†åŒ–æ£€æµ‹ç»“æœ"""
    # åˆ›å»ºå±å¹•çš„å‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹å›¾åƒ
    display_img = screen.copy()
    screen_height, screen_width = display_img.shape[:2]
    
    # ç±»åˆ«é¢œè‰²æ˜ å°„
    class_colors = {
        0: (0, 0, 255),      # circle - çº¢è‰²
        1: (0, 255, 0),      # slider - ç»¿è‰²
        2: (255, 0, 0)       # spinner - è“è‰²
    }
    
    # ç»˜åˆ¶æ£€æµ‹ç»“æœ
    for result in results:
        # ç¼©æ”¾è¾¹ç•Œæ¡†ä»¥åŒ¹é…åŸå§‹å±å¹•å°ºå¯¸
        box = result['box']
        x1 = int(box[0] * screen_width / CONFIG['image_size'][0])
        y1 = int(box[1] * screen_height / CONFIG['image_size'][1])
        x2 = int(box[2] * screen_width / CONFIG['image_size'][0])
        y2 = int(box[3] * screen_height / CONFIG['image_size'][1])
        
        # è·å–ç±»åˆ«å’Œåˆ†æ•°
        class_id = result['class']
        score = result['score']
        class_name = CONFIG['class_names'][class_id] if class_id < len(CONFIG['class_names']) else f'Class {class_id}'
        color = class_colors.get(class_id, (255, 255, 0))  # é»˜è®¤é»„è‰²
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(display_img, (x1, y1), (x2, y2), color, 2)
        
        # æ·»åŠ æ ‡ç­¾
        label = f'{class_name}: {score:.2f}'
        cv2.putText(display_img, label, (x1, max(y1-10, 10)), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    
    # æ˜¾ç¤ºç»“æœ
    cv2.imshow('Osu Object Detection', display_img)
    cv2.waitKey(1)  # ä¿æŒçª—å£å“åº”

def detect_object_position(screen):
    """ä½¿ç”¨å›¾åƒå¤„ç†æŠ€æœ¯æ£€æµ‹å±å¹•ä¸­ç‰©ä½“çš„å®é™…ä½ç½®"""
    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)
    
    # åº”ç”¨é˜ˆå€¼å¤„ç†ï¼Œçªå‡ºæ˜äº®çš„ç‰©ä½“
    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
    
    # å¯»æ‰¾è½®å»“
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        # æ‰¾åˆ°æœ€å¤§çš„è½®å»“ï¼ˆå‡è®¾æ˜¯æˆ‘ä»¬è¦æ£€æµ‹çš„ç‰©ä½“ï¼‰
        largest_contour = max(contours, key=cv2.contourArea)
        
        # è®¡ç®—è½®å»“çš„è¾¹ç•Œæ¡†
        x, y, w, h = cv2.boundingRect(largest_contour)
        
        # è®¡ç®—ä¸­å¿ƒç‚¹
        center_x = x + w // 2
        center_y = y + h // 2
        
        return center_x, center_y
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è½®å»“ï¼Œè¿”å›å±å¹•ä¸­å¿ƒ
    return screen.shape[1] // 2, screen.shape[0] // 2

def run(model: nn.Module):
    """è¿è¡Œå®æ—¶æ£€æµ‹å¾ªç¯"""
    is_run = False
    should_exit = False
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    model.to(device)
    model.eval()
    
    print("å®æ—¶æ£€æµ‹å·²å¯åŠ¨ï¼æŒ‰ 'q' é”®åˆ‡æ¢çŠ¶æ€ï¼Œé•¿æŒ‰ 'q' 2ç§’é€€å‡º")
    
    screen_width, screen_height = pyautogui.size()
    slider_hold = False
    spinner_hold = False
    
    # å®šä¹‰qé”®æŒ‰ä¸‹å¤„ç†å‡½æ•°
    def on_q_press(event):
        nonlocal is_run, slider_hold, spinner_hold, should_exit
        # åªå¤„ç†æŒ‰ä¸‹äº‹ä»¶
        if event.name == 'q' and event.event_type == 'down':
            # è®°å½•æŒ‰é”®æ—¶é—´
            start_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é•¿æŒ‰
            while keyboard.is_pressed('q'):
                if time.time() - start_time > 2:
                    # é•¿æŒ‰2ç§’é€€å‡º
                    print("æ£€æµ‹åˆ°é•¿æŒ‰ï¼Œç¨‹åºé€€å‡º")
                    should_exit = True
                    return 'quit'
                time.sleep(0.01)
            
            # å¦‚æœä¸æ˜¯é•¿æŒ‰ï¼Œè§†ä¸ºçŸ­æŒ‰
            if not should_exit:  # ç¡®ä¿åªåœ¨éé€€å‡ºæƒ…å†µä¸‹åˆ‡æ¢çŠ¶æ€
                if is_run:
                    is_run = False
                    print("æ£€æµ‹å·²æš‚åœï¼Œå†æ¬¡æŒ‰ 'q' ç»§ç»­")
                    cv2.destroyAllWindows()
                    # ç¡®ä¿é‡Šæ”¾é¼ æ ‡
                    pyautogui.mouseUp()
                    slider_hold = False
                    spinner_hold = False
                else:
                    is_run = True
        return None
    
    # æ³¨å†ŒæŒ‰é”®äº‹ä»¶å¤„ç†å™¨
    keyboard.hook(on_q_press)
    
    try:
        while not should_exit:
            if is_run:
                # 1. æ•è·å½“å‰å±å¹•
                screen = capture_screen()
                
                # ç®€åŒ–å›¾åƒå¤„ç†æµç¨‹
                # è½¬æ¢ä¸ºç°åº¦å›¾
                screen = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)
                #ç§»é™¤é»‘è‰²èƒŒæ™¯
                screen[screen < 10] = 0
                # ç¡®ä¿æ˜¯3é€šé“å›¾åƒ
                screen = cv2.cvtColor(screen, cv2.COLOR_GRAY2RGB)
                
                # 2. é¢„å¤„ç†å›¾åƒ
                processed_img = preprocess_image(screen).to(device)
                
                # 3. è¿›è¡Œæ¨ç†
                with torch.no_grad():
                    predictions = model(processed_img.unsqueeze(0))
                
                # 4. è§£æé¢„æµ‹ç»“æœ
                results = parse_predictions(predictions, device)
                
                if results:
                    # ç›´æ¥ä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœï¼Œä¸éœ€è¦æ’åº
                    best_result = results[0]
                    current_class = best_result['class']
                    current_score = best_result['score']
                    
                    # ä½¿ç”¨å›¾åƒå¤„ç†æ£€æµ‹å®é™…ç‰©ä½“ä½ç½®
                    object_x, object_y = detect_object_position(screen)
                    object_x = int(object_x * screen_width / screen.shape[1])
                    object_y = int(object_y * screen_height / screen.shape[0])
                    
                    # é‡ç½®é¼ æ ‡çŠ¶æ€ï¼Œå¦‚æœå½“å‰æ²¡æœ‰æ£€æµ‹åˆ°é«˜ç½®ä¿¡åº¦çš„ç›®æ ‡
                    if current_score < 0.7:
                        if slider_hold:
                            pyautogui.mouseUp()
                            slider_hold = False
                        if spinner_hold:
                            pyautogui.mouseUp()
                            spinner_hold = False
                    else:
                        # å¤„ç†ä¸åŒç±»å‹çš„ç›®æ ‡
                        if current_class == 0:  # circle
                            # ç¡®ä¿å…¶ä»–çŠ¶æ€è¢«é‡ç½®
                            if slider_hold:
                                pyautogui.mouseUp()
                                slider_hold = False
                            if spinner_hold:
                                pyautogui.mouseUp()
                                spinner_hold = False
                            
                            try:
                                pyautogui.click(object_x, object_y)
                            except Exception:
                                pass
                        
                        elif current_class == 1:  # slider
                            # ç¡®ä¿spinnerçŠ¶æ€è¢«é‡ç½®
                            if spinner_hold:
                                pyautogui.mouseUp()
                                spinner_hold = False
                            
                            try:
                                if not slider_hold:
                                    pyautogui.mouseDown(object_x, object_y)
                                    slider_hold = True
                                else:
                                    pyautogui.moveTo(object_x, object_y)
                            except Exception:
                                if slider_hold:
                                    pyautogui.mouseUp()
                                    slider_hold = False
                        
                        elif current_class == 2:  # spinner
                            # ç¡®ä¿sliderçŠ¶æ€è¢«é‡ç½®
                            if slider_hold:
                                pyautogui.mouseUp()
                                slider_hold = False
                            
                            try:
                                if not spinner_hold:
                                    pyautogui.mouseDown(object_x, object_y)
                                    spinner_hold = True
                                else:
                                    # å®ç°å¿«é€Ÿæ—‹è½¬ - æé«˜æ—‹è½¬é€Ÿåº¦
                                    current_time = 90  # æ›´é«˜çš„æ—‹è½¬é€Ÿåº¦
                                    circle_radius = 30  # å¢å¤§æ—‹è½¬åŠå¾„
                                    spin_x = int(object_x + circle_radius * math.sin(current_time))
                                    spin_y = int(object_y + circle_radius * math.cos(current_time))
                                    
                                    # ç§»åŠ¨é¼ æ ‡ï¼Œä½¿ç”¨ç§»åŠ¨æŒç»­æ—¶é—´ä¸º0ä»¥è·å¾—æœ€å¿«é€Ÿåº¦
                                    pyautogui.moveTo(spin_x, spin_y)
                                    current_time *= 1.5
                            except Exception:
                                if spinner_hold:
                                    pyautogui.mouseUp()
                                    spinner_hold = False
                        elif current_class == 3:
                            pass
                        else:
                            # é‡ç½®æ‰€æœ‰çŠ¶æ€
                            pyautogui.mouseUp()
                            slider_hold = False   
                            spinner_hold = False 
                else:
                    # æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ï¼Œé‡ç½®é¼ æ ‡çŠ¶æ€
                    if slider_hold:
                        pyautogui.mouseUp()
                        slider_hold = False
                    if spinner_hold:
                        pyautogui.mouseUp()
                        spinner_hold = False
                # ç§»é™¤å»¶è¿Ÿï¼Œæœ€å¤§åŒ–å¤„ç†é€Ÿåº¦
                # time.sleep(0.0001)
    except KeyboardInterrupt:
        pass
    finally:
        # æ³¨é”€é”®ç›˜é’©å­
        keyboard.unhook_all()
        # ç¡®ä¿é‡Šæ”¾é¼ æ ‡
        pyautogui.mouseUp()
        cv2.destroyAllWindows()
        print("å®æ—¶æ£€æµ‹å·²ç»“æŸ")

# å¦‚æœä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œæ‰§è¡Œä¸»å‡½æ•°
if __name__ == '__main__':
    main()

