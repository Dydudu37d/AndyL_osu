import torch
import cv2
import numpy as np
import argparse
import random
import os
import matplotlib.pyplot as plt
from collections import OrderedDict

# å¼•å…¥ä½ çš„é¡¹ç›®é…ç½®
from config import Config
from model import OsuIDM

def load_model(checkpoint_path, device):
    """åŠ è½½æ¨¡å‹æƒé‡ï¼Œè‡ªåŠ¨å¤„ç† DDP çš„ 'module.' å‰ç¼€"""
    print(f"ğŸ”„ Loading model from {checkpoint_path}...")
    
    # åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    model = OsuIDM().to(device)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # å…¼å®¹å¤„ç†ï¼šæ£€æŸ¥æ˜¯å­˜çš„æ•´ä¸ª checkpoint å­—å…¸è¿˜æ˜¯åªæœ‰ state_dict
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    else:
        state_dict = checkpoint
        
    # å»é™¤ DDP è®­ç»ƒäº§ç”Ÿçš„ 'module.' å‰ç¼€
    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k.replace('module.', '') # remove `module.`
        new_state_dict[name] = v
        
    model.load_state_dict(new_state_dict)
    model.eval()
    return model

def preprocess_frames(frames):
    """
    å°†è¯»å–çš„ OpenCV å¸§åˆ—è¡¨è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥çš„ Tensor
    é€»è¾‘éœ€ä¸¥æ ¼å¯¹é½ train.py / preprocess.py
    """
    processed_frames = []
    
    for frame in frames:
        # 1. Resize (Config.IMG_SIZE = 224)
        frame_resized = cv2.resize(frame, (Config.IMG_SIZE, Config.IMG_SIZE))
        
        # 2. ç°åº¦åŒ–
        frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)
        processed_frames.append(frame_gray)
    
    # 3. å †å  (Sequence Length = 6) -> Shape: (6, 224, 224)
    frame_stack = np.array(processed_frames)
    
    # 4. å½’ä¸€åŒ– (0-255 -> 0.0-1.0) å¯¹åº” train.py ä¸­çš„ float() / 255.0
    tensor = torch.from_numpy(frame_stack).float() / 255.0
    
    # 5. å¢åŠ  Batch ç»´åº¦ -> Shape: (1, 6, 224, 224)
    tensor = tensor.unsqueeze(0)
    
    return tensor, frame_stack

def visualize_result(frame_stack, pred_mouse, pred_click, save_path="result.jpg"):
    """
    å¯è§†åŒ–é€»è¾‘ï¼š
    1. 6å¸§å åŠ  (æ¯å¸§ 1/6 é€æ˜åº¦)
    2. ç”»ç®­å¤´ (ç§»åŠ¨)
    3. ç”»çº¢ç‚¹ (ç‚¹å‡»)
    """
    H, W = frame_stack.shape[1], frame_stack.shape[2] # 224, 224
    
    # --- 1. åˆ¶ä½œå åŠ èƒŒæ™¯ ---
    # è®¡ç®—å¹³å‡å€¼æ¥å®ç° 1/6 é€æ˜åº¦å åŠ æ•ˆæœ
    composite_gray = np.mean(frame_stack, axis=0).astype(np.uint8)
    # è½¬å› BGR ä»¥ä¾¿ç”»å½©è‰²çº¿
    vis_img = cv2.cvtColor(composite_gray, cv2.COLOR_GRAY2BGR)
    
    # --- 2. è§£æé¢„æµ‹ç»“æœ ---
    # pred_mouse æ˜¯å½’ä¸€åŒ–çš„ dx, dy (åŸºäº 512x384)
    # æˆ‘ä»¬éœ€è¦å°†å…¶ç¼©æ”¾åˆ°å½“å‰å›¾ç‰‡å°ºå¯¸ (224x224) ä»¥ä¾¿å¯è§†åŒ–
    dx_norm, dy_norm = pred_mouse[0], pred_mouse[1]
    click_prob = pred_click
    
    # è¿˜åŸåˆ°åŸå§‹ osu åæ ‡ç³»çš„ä½ç§» (512x384)
    real_dx = dx_norm * 512.0
    real_dy = dy_norm * 384.0
    
    # æ˜ å°„åˆ°å¯è§†åŒ–å›¾ç‰‡çš„å°ºå¯¸ (è¿™é‡Œä¸ºäº†æ˜¾ç¤ºæ˜æ˜¾ï¼Œç¨å¾®æ”¾å¤§ä¸€ç‚¹æ¯”ä¾‹ï¼Œæˆ–è€…ç›´æ¥æŒ‰æ¯”ä¾‹æ˜ å°„)
    # å›¾ç‰‡å®½ 224ï¼Œosu å®½ 512 -> æ¯”ä¾‹çº¦ 0.43
    scale_x = W / 512.0
    scale_y = H / 384.0
    
    vis_dx = int(real_dx * scale_x * 5.0) # *5 æ˜¯ä¸ºäº†è®©å¾®å°çš„ç§»åŠ¨åœ¨å›¾ä¸Šè‚‰çœ¼æ›´æ˜æ˜¾
    vis_dy = int(real_dy * scale_y * 5.0)
    
    # è®¾å®šä¸­å¿ƒç‚¹
    center_x, center_y = W // 2, H // 2
    end_x, end_y = center_x + vis_dx, center_y + vis_dy
    
    # --- 3. ç»˜åˆ¶ ---
    
    # A. ç»˜åˆ¶ç§»åŠ¨ç®­å¤´ (ç»¿è‰²)
    # æç¤ºï¼šIDM æ¨¡å‹é¢„æµ‹çš„æ˜¯â€œå…‰æ ‡çš„ç›¸å¯¹ç§»åŠ¨(Velocity)â€ï¼Œè€Œä¸æ˜¯ç»å¯¹ä½ç½®
    # æ‰€ä»¥æˆ‘ä»¬ä»ç”»é¢ä¸­å¿ƒç”»å‡ºè¿™ä¸ªå‘é‡
    cv2.arrowedLine(vis_img, (center_x, center_y), (end_x, end_y), (0, 255, 0), 2, tipLength=0.3)
    
    # B. ç»˜åˆ¶ç‚¹å‡»çŠ¶æ€
    is_clicking = click_prob > 0.5
    status_text = f"Click: {click_prob:.2f}"
    
    if is_clicking:
        # å¦‚æœç‚¹å‡»ï¼Œåœ¨ä¸­å¿ƒç”»ä¸€ä¸ªçº¢è‰²çš„å®å¿ƒåœ†
        cv2.circle(vis_img, (center_x, center_y), 10, (0, 0, 255), -1) 
        text_color = (0, 0, 255) # Red
    else:
        # æ²¡ç‚¹å‡»ï¼Œç”»ä¸€ä¸ªç©ºå¿ƒè“åœ†
        cv2.circle(vis_img, (center_x, center_y), 10, (255, 0, 0), 1)
        text_color = (255, 0, 0) # Blue
        
    # C. æ·»åŠ æ–‡å­—ä¿¡æ¯
    cv2.putText(vis_img, f"dx:{dx_norm:.3f} dy:{dy_norm:.3f}", (5, 20), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
    cv2.putText(vis_img, status_text, (5, 40), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1)

    # ä¿å­˜
    cv2.imwrite(save_path, vis_img)
    print(f"âœ… Result saved to {save_path}")
    print(f"   Pred: Move({dx_norm:.4f}, {dy_norm:.4f}), Click({click_prob:.4f})")

def main():
    parser = argparse.ArgumentParser(description="Osu IDM Inference Tool")
    parser.add_argument("--video", type=str, required=True, help="Path to mp4 video")
    parser.add_argument("--checkpoint", type=str, required=True, help="Path to .pth checkpoint")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    args = parser.parse_args()

    # 1. å‡†å¤‡æ¨¡å‹
    model = load_model(args.checkpoint, args.device)

    # 2. è¯»å–è§†é¢‘å¹¶éšæœºæŠ½æ ·
    if not os.path.exists(args.video):
        print("âŒ Video file not found.")
        return

    cap = cv2.VideoCapture(args.video)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames < 10:
        print("âŒ Video too short.")
        return

    # éšæœºé€‰æ‹©ä¸€ä¸ªèµ·å§‹ç‚¹ (ç¡®ä¿åé¢æœ‰6å¸§)
    start_idx = random.randint(0, total_frames - 7)
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_idx)
    
    frames = []
    print(f"ğŸ¬ Sampling 6 frames starting from index {start_idx}...")
    for _ in range(6):
        ret, frame = cap.read()
        if ret:
            frames.append(frame)
        else:
            break
    cap.release()
    
    if len(frames) != 6:
        print("âŒ Failed to read 6 consecutive frames.")
        return

    # 3. é¢„å¤„ç†
    input_tensor, frame_stack = preprocess_frames(frames)
    input_tensor = input_tensor.to(args.device)

    # 4. æ¨ç†
    with torch.no_grad():
        # æ¨¡å‹è¾“å‡º: mouse -> (B, 2), click -> (B, 1)
        pred_mouse, pred_click = model(input_tensor)
        
        # è·å–æ•°å€¼
        dx_dy = pred_mouse[0].cpu().numpy() # [dx, dy]
        click_logit = pred_click[0].item()
        click_prob = torch.sigmoid(pred_click[0]).item()

    # 5. å¯è§†åŒ–
    visualize_result(frame_stack, dx_dy, click_prob, save_path=f"inference_frame_{start_idx}.jpg")

if __name__ == "__main__":
    main()