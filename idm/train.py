import os
import glob
import re
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler, random_split
from torch.cuda.amp import autocast, GradScaler
import numpy as np
from tqdm import tqdm

from config import Config
from model import OsuIDM

def setup_ddp():
    # SAI é…åˆ Slurm ä¼šè‡ªåŠ¨è®¾ç½® MASTER_ADDR, MASTER_PORT, RANK, WORLD_SIZE ç­‰ç¯å¢ƒå˜é‡
    dist.init_process_group(backend="nccl")
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    return local_rank

class FastOsuDataset(Dataset):
    def __init__(self, memmap_path, meta_path):
        meta = np.load(meta_path)
        self.total_samples = int(meta['total'])
        self.shape = tuple(meta['shape'])
        self.actions = torch.from_numpy(meta['actions'].astype(np.float32))
        # ä½¿ç”¨ mode='r' é˜²æ­¢æ„å¤–ä¿®æ”¹æ•°æ®
        self.data_memmap = np.memmap(memmap_path, dtype=np.uint8, mode='r', shape=(self.total_samples, *self.shape))

    def __len__(self):
        return self.total_samples

    def __getitem__(self, idx):
        # å¿…é¡» copyï¼Œå¦åˆ™è½¬ Tensor å¯èƒ½æŠ¥é”™æˆ–äº§ç”Ÿè´Ÿ stride é—®é¢˜
        seq_data = np.array(self.data_memmap[idx], copy=True)
        seq_tensor = torch.from_numpy(seq_data).float() / 255.0
        label_tensor = self.actions[idx]
        return seq_tensor, label_tensor

def find_latest_checkpoint(checkpoint_dir):
    """æŸ¥æ‰¾ç›®å½•ä¸­æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    if not os.path.exists(checkpoint_dir):
        return None, 0
    
    # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º "osu_ddp_epoch_{epoch}.pth"
    files = glob.glob(os.path.join(checkpoint_dir, "osu_ddp_epoch_*.pth"))
    if not files:
        return None, 0
    
    # æå– epoch æ•°å­—å¹¶æ’åº
    latest_file = max(files, key=lambda x: int(re.search(r'epoch_(\d+)', x).group(1)))
    latest_epoch = int(re.search(r'epoch_(\d+)', latest_file).group(1))
    return latest_file, latest_epoch

def validate(model, val_loader, criterion_mouse, criterion_click, local_rank):
    """éªŒè¯å¾ªç¯"""
    model.eval()
    total_loss = torch.tensor(0.0).to(local_rank)
    count = torch.tensor(0.0).to(local_rank)
    
    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs = imgs.to(local_rank, non_blocking=True)
            labels = labels.to(local_rank, non_blocking=True)
            
            # éªŒè¯é›†é€šå¸¸ä¸éœ€è¦ autocastï¼Œé™¤éæ˜¾å­˜éå¸¸ç´§å¼ ï¼Œä½†ä¿æŒä¸€è‡´è¾ƒå¥½
            with autocast():
                pred_mouse, pred_click = model(imgs)
                loss = (criterion_mouse(pred_mouse, labels[:, :2]) * Config.MOUSE_LOSS_WEIGHT) + \
                       (criterion_click(pred_click, labels[:, 2:3]) * Config.CLICK_LOSS_WEIGHT)
            
            total_loss += loss
            count += 1
    
    # æ±‡æ€»æ‰€æœ‰è¿›ç¨‹çš„ Loss
    dist.all_reduce(total_loss, op=dist.ReduceOp.SUM)
    dist.all_reduce(count, op=dist.ReduceOp.SUM)
    
    avg_loss = total_loss / count
    return avg_loss.item()

def save_checkpoint(model, optimizer, scheduler, scaler, epoch, path):
    """ä¿å­˜åŒ…å«æ‰€æœ‰çŠ¶æ€çš„æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.module.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'scaler_state_dict': scaler.state_dict()
    }
    torch.save(checkpoint, path)

def train():
    local_rank = setup_ddp()
    is_main_process = (dist.get_rank() == 0)
    
    if is_main_process:
        os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)
        print(f"ğŸ”¥ Distributed training started on {dist.get_world_size()} GPUs")

    # 1. åŠ è½½æ•°æ®é›†
    dataset = FastOsuDataset(Config.MEMMAP_PATH, Config.META_PATH)
    
    # 2. æ•°æ®åˆ‡åˆ† (FIX: ä½¿ç”¨å›ºå®šç§å­ç¡®ä¿æ‰€æœ‰ rank åˆ‡åˆ†ä¸€è‡´)
    train_size = int(0.98 * len(dataset))
    val_size = len(dataset) - train_size
    generator = torch.Generator().manual_seed(42) # å›ºå®šç§å­
    train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=generator)
    
    # è®­ç»ƒé›† Sampler
    train_sampler = DistributedSampler(train_ds, shuffle=True)
    # éªŒè¯é›† Sampler (shuffle=False, ç¡®ä¿éªŒè¯ç»“æœç¨³å®š)
    val_sampler = DistributedSampler(val_ds, shuffle=False)
    
    train_loader = DataLoader(
        train_ds, 
        batch_size=Config.BATCH_SIZE, 
        sampler=train_sampler,
        num_workers=Config.NUM_WORKERS,
        pin_memory=True,
        persistent_workers=True
    )
    
    val_loader = DataLoader(
        val_ds,
        batch_size=Config.BATCH_SIZE,
        sampler=val_sampler,
        num_workers=Config.NUM_WORKERS,
        pin_memory=True,
        persistent_workers=True
    )

    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = OsuIDM().to(local_rank)
    model = DDP(model, device_ids=[local_rank])
    
    optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)
    
    # OneCycleLR éœ€è¦å®Œæ•´çš„ steps æ•°é‡
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=Config.LEARNING_RATE, 
        steps_per_epoch=len(train_loader), 
        epochs=Config.EPOCHS
    )
    
    scaler = GradScaler()
    criterion_mouse = nn.MSELoss()
    criterion_click = nn.BCEWithLogitsLoss()

    # 4. å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    start_epoch = 0
    latest_checkpoint_path, latest_epoch = find_latest_checkpoint(Config.CHECKPOINT_DIR)
    
    if latest_checkpoint_path:
        if is_main_process:
            print(f"ğŸ”„ Resuming from checkpoint: {latest_checkpoint_path} (Epoch {latest_epoch})")
        
        # map_location å¿…é¡»æŒ‡å®šä¸ºå½“å‰ GPUï¼Œé˜²æ­¢çˆ†æ˜¾å­˜
        map_location = {'cuda:%d' % 0: 'cuda:%d' % local_rank}
        checkpoint = torch.load(latest_checkpoint_path, map_location=map_location)
        
        # å…¼å®¹æ—§ç‰ˆæœ¬ï¼ˆå¦‚æœä¹‹å‰åªä¿å­˜äº† model state dictï¼‰
        if 'model_state_dict' in checkpoint:
            model.module.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            scaler.load_state_dict(checkpoint['scaler_state_dict'])
            start_epoch = checkpoint['epoch'] # ä»ä¿å­˜çš„ epoch å¼€å§‹ï¼Œæ„å‘³ç€è¯¥ epoch å·²å®Œæˆï¼Œæ¥ä¸‹æ¥è·‘ epoch+1
            
            # å¦‚æœ OneCycleLR æ˜¯æŒ‰ step èµ°çš„ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦æ‰‹åŠ¨ step scheduler åˆ°æ­£ç¡®ä½ç½®
            # ä½† pytorch çš„ load_state_dict é€šå¸¸ä¼šå¤„ç†å¥½ last_epoch
        else:
            # å…¼å®¹ä»¥å‰åªä¿å­˜äº† state_dict çš„æƒ…å†µ
            if is_main_process:
                print("âš ï¸ Found legacy checkpoint (weights only). Optimizer state will be reset.")
            model.module.load_state_dict(checkpoint)
            start_epoch = latest_epoch # å‡è®¾æ–‡ä»¶åé‡Œçš„ epoch æ˜¯å·²ç»è·‘å®Œçš„

    # 5. è®­ç»ƒå¾ªç¯
    # å¦‚æœ start_epoch = 10ï¼Œrange(10, 100) ä¼šä» Epoch 11 å¼€å§‹è·‘ï¼ˆæ‰“å°æ˜¾ç¤ºä¸º Epoch 11ï¼‰
    for epoch in range(start_epoch, Config.EPOCHS):
        train_sampler.set_epoch(epoch)
        model.train()
        
        loader = tqdm(train_loader, desc=f"Train Epoch {epoch+1}") if is_main_process else train_loader
        
        for imgs, labels in loader:
            imgs, labels = imgs.to(local_rank, non_blocking=True), labels.to(local_rank, non_blocking=True)
            
            with autocast():
                pred_mouse, pred_click = model(imgs)
                loss = (criterion_mouse(pred_mouse, labels[:, :2]) * Config.MOUSE_LOSS_WEIGHT) + \
                       (criterion_click(pred_click, labels[:, 2:3]) * Config.CLICK_LOSS_WEIGHT)
            
            optimizer.zero_grad(set_to_none=True)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

        # 6. éªŒè¯å¾ªç¯ (æ¯ä¸ª epoch ç»“æŸ)
        if is_main_process:
            print(f"ğŸ” Validating Epoch {epoch+1}...")
        
        val_loss = validate(model, val_loader, criterion_mouse, criterion_click, local_rank)
        
        if is_main_process:
            print(f"ğŸ“‰ Epoch {epoch+1} | Val Loss: {val_loss:.6f}")

            # 7. ä¿å­˜æ£€æŸ¥ç‚¹ (ä½¿ç”¨æ–°æ ¼å¼)
            save_path = os.path.join(Config.CHECKPOINT_DIR, f"osu_ddp_epoch_{epoch+1}.pth")
            save_checkpoint(model, optimizer, scheduler, scaler, epoch+1, save_path)
            
            # å¯é€‰ï¼šæ¸…ç†æ—§çš„ checkpoint ä»¥èŠ‚çœç©ºé—´
            # prev_path = os.path.join(Config.CHECKPOINT_DIR, f"osu_ddp_epoch_{epoch}.pth")
            # if os.path.exists(prev_path): os.remove(prev_path)

    dist.destroy_process_group()

if __name__ == "__main__":
    train()